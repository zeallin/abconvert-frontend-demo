from bs4 import BeautifulSoup
import json
import requests
import time
import os
import re
import base64
from pathlib import Path


TMDB_API_KEY = "2b9c47fb345738c2a2e84420da94ed59"  


## this script is generated by ChatGPT with prompt, with some post modifications:
# write a script to parse html content, find items start with "<div class="collection-item" data-product="true">" and extract following information inside as JSON item:
# 1.  href value of <a> tag with class="collection-item__details" -> link
# 2. inner html of <p> tag with class="collection-item__sku" -> id
# 3. inner html of <h3> tag -> name
# 4. <p> tag with class="collection-item__poster-details", will have content like : 27" × 40", Original<span>, 2025</span>, where 27" -> sizeWidthInch, 40" -> sizeHeightInch, 2025 -> year
# 5. inner html of <span> tag with class="ss__price__value" -> price


# Path to your input HTML file
HTML_FILE_PATH = "./base"
PROCESS_TYPE = "top_selling"
IMG_FILE_PATH = "./pictures/thumb"

def list_html_files(path):
    # List all files in the directory
    files = os.listdir(path)
    # Filter for .html files
    html_files = [f for f in files if f.endswith('.html')]
    return html_files

def generate_slug(text):
    # Convert to lowercase
    text = text.lower()
    # Replace spaces and special characters with hyphens
    text = re.sub(r'[^a-z0-9]+', '-', text)
    # Remove leading/trailing hyphens
    text = text.strip('-')
    return text

def get_movie_desc(name):
    print(name)
    # need to sleep for 500ms to prevent block
    time.sleep(0.5) 

    search_url = "https://api.themoviedb.org/3/search/movie"
    params = {
        "api_key": TMDB_API_KEY,
        "query": name
    }
    search_response = requests.get(search_url, params=params).json()

    if search_response["results"]:
        # 2. Get the first matching movie's ID
        movie_id = search_response["results"][0]["id"]

        # 3. Fetch movie details
        details_url = f"https://api.themoviedb.org/3/movie/{movie_id}"
        details_params = {
            "api_key": TMDB_API_KEY
        }
        details_response = requests.get(details_url, params=details_params).json()

        # 4. Print out the description
        # title = details_response.get("title", "Unknown Title")
        desc = details_response.get("overview", "No description available.")

def save_base64_image(src_string, output_path):
    # Regular expression to extract mime type and base64 data
    pattern = r'data:image/(.*?);base64,(.*)'
    match = re.match(pattern, src_string)
    
    if not match:
        raise ValueError("Invalid base64 image src format")

    # Extract mime type (e.g., 'jpeg', 'png') and base64 data
    mime_type, base64_data = match.groups()
    
    # Map mime type to file extension
    extension_map = {
        'jpeg': '.jpg',
        'png': '.png',
        'gif': '.gif',
        'webp': '.webp'
    }
    extension = extension_map.get(mime_type, '.bin')  # Default to .bin if unknown
    
    # Decode base64 data to binary
    image_data = base64.b64decode(base64_data)
    
    # Ensure output path has the correct extension
    output_file = Path(output_path).with_suffix(extension)
    
    # Write binary data to file
    with open(output_file, 'wb') as f:
        f.write(image_data)
    
    return os.path.basename(output_file)

# Read the HTML content from the file
def parse_html_content(html_path, parsed_data, detail_links):
    with open(html_path, "r", encoding="utf-8") as file:
        html_content = file.read()

    # Parse the HTML content
    soup = BeautifulSoup(html_content, "html.parser")
    items = soup.find_all("div", class_="collection-item", attrs={"data-product": "true"})

    is_get_desc = False

    for item in items:
        link_tag = item.find("a", class_="collection-item__details")
        sku_tag = item.find("p", class_="collection-item__sku")
        name_tag = item.find("h3")
        details_tag = item.find("p", class_="collection-item__poster-details")
        price_tag = item.find("span", class_="ss__price__value")
        picture_tag = item.find("picture", class_="picture")
        img_tag = picture_tag.find("img", class_="ss_img_load")

        # print(img_tag.get('src'))

        if img_tag.get('src') is None:
            continue

        # Save image
        img_file_name = generate_slug(name_tag.text) + "_" + sku_tag.text.strip().lower()
        print(img_file_name)
        # print(img_tag['src'])
        thumb_path = save_base64_image(img_tag['src'], os.path.join(IMG_FILE_PATH, img_file_name))
                          
        # Extract poster details
        width = height = year = None
        if details_tag:
            # Extract year from span
            span_tag = details_tag.find("span")
            if span_tag:
                year = span_tag.text.strip().lstrip(", ")

            # Remove span for raw text parsing
            for span in details_tag.find_all("span"):
                span.extract()
            details_text = details_tag.get_text(separator=" ", strip=True)

            parts = details_text.split(',')
            if len(parts) >= 1:
                size_part = parts[0].replace('"', '').replace('×', 'x').strip()
                size_split = size_part.split('x')
                if len(size_split) == 2:
                    width = size_split[0].strip()
                    height = size_split[1].strip()


        desc = None

        if is_get_desc:
            desc = get_movie_desc(name)

        link = link_tag.get("href") if link_tag else None
        parsed_data.append({
            "link": link,
            "id": sku_tag.text.strip() if sku_tag else None,
            "name": name_tag.text.strip() if name_tag else None,
            "sizeWidthInch": float(width) if width else None,
            "sizeHeightInch": float(height) if height else None,
            "year": int(year) if year else None,
            "price": float(price_tag.text.strip().replace(',', '')) if price_tag else None,
            "desc": desc,
            "thumbPath": thumb_path
        })

        detail_links.append(link)


parsed_data = []
detail_links = []

base_path = os.path.join(HTML_FILE_PATH, PROCESS_TYPE)
html_files = list_html_files(base_path)
for file in html_files:
    print("Processing: " + file)
    parse_html_content(os.path.join(base_path, file), parsed_data, detail_links)

# Save JSON to a file
with open(Path(PROCESS_TYPE + "_base").with_suffix(".json"), "w", encoding="utf-8") as out_file:
    json.dump(parsed_data, out_file, indent=2, ensure_ascii=False)

# Save URL list to a file
with open(Path(PROCESS_TYPE + "_link").with_suffix(".txt"), "w", encoding="utf-8") as txt_file:
    for line in detail_links:
        txt_file.write(line + "\n") # works with any number of elements in a line
